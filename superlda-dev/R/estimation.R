# R functions for training and inference of supervised LDA models

#' Train a labeled (or "flat") LDA model
#'
#' Trains the "flat" LDA model via collapsed Gibbs sampling.
#' @param slda_obj A superlda data object (see \link{convert_superlda} for more information on superlda data objects).
#' @param beta Hyperparameter for the token distribution (see Details for more information; defaults to beta = 0.01).
#' @param alpha Hyperparameter for the document distribution (see Details for more information; defaults to alpha = 50).
#' @param niter Number of iterations to use for the Gibbs sampler (defaults to niter = 10).
#' @return Returns a list with the two matrices of interest:
#' * `word_probs` -- Normalized probability matrix for words (tokens x labels)
#' * `doc_probs` -- Normalized probability matrix for documents (docs x labels)
#' @author Travis G. Coan <t.coan@exeter.ac.uk>
#' @references
#' Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: a supervised topic model for credit attribution in multi-labeled corpora.
#' In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1 (EMNLP '09), Vol. 1.
#' Association for Computational Linguistics, Stroudsburg, PA, USA, 248-256.
#'
#' Timothy Rubin, America Chambers, Padhraic Smyth, and Mark Steyvers. 2012. Statistical Topic Models for Multi-Label Document Classification.
#' *Jounral of Machine Learning* 88: 157-208.
#' @examples
#' # Load data table
#' dat = load("election_media.Rdata")
#'
#' # Construct superlda data object using defaults:
#' slda_obj = construct_superlda(dat)
#'
#' # Estimate a labeled (or flat) LDA using the defaults:
#' fit = train_flat_lda
#' @export
train_flat_lda <- function(slda_obj, beta = .01, alpha = 50, niter = 10){
  # Initialize
  ndocs <- slda_obj$dtm@Dim[[1]]
  nunique <- slda_obj$dtm@Dim[[2]]
  ntokens <- sum(slda_obj$dtm@x)
  nelem <- length(slda_obj$dtm@i)
  return(train_label_word(slda_obj$dtm, slda_obj$dlm, ndocs, nunique, ntokens, nelem, niter, beta, alpha))
}


#' Infer labels for unseen documents via the "flat" (or labeled) LDA model
#'
#' Infers the labels for unseen documents using a trained "flat" (or labeled) LDA model.
#' The inferencer uses collapsed Gibbs sampling, but fixes the word-label distributions
#' to the trained values.
#' @param corpus A corpus list object generated by the read_corpus() function
#' @author Travis G. Coan <t.coan@exeter.ac.uk>
#' @examples
#' read_corpus("data/text_to_read.txt")
#' @export
infer_flat_lda <- function(slda_obj, word_probs, beta = 1, alpha = 100, niter = 10){
  # Initialize inputs
  ndocs <- slda_obj$dtm@Dim[[1]]
  nunique <- slda_obj$dtm@Dim[[2]]
  ntokens <- sum(slda_obj$dtm@x)
  nelem <- length(slda_obj$dtm@i)
  nlabs <- ncol(word_probs)
  return(infer_labels(slda_obj$dtm, ndocs, nunique, ntokens, nelem, nlabs, niter, beta, alpha, word_probs))
}

#' Infer topic assignments for unseen documents using a fitted LDAMallet model.
#'
#' Infers topic assignments for tokens in unseen documents using a fitted LDAMallet model.
#' The inferencer uses collapsed Gibbs sampling, but fixes the word-label distributions
#' to the trained values.
#' @param corpus A corpus list object generated by the read_corpus() function
#' @author Travis G. Coan <t.coan@exeter.ac.uk>
#' @examples
#' read_corpus("data/text_to_read.txt")
#' @export
infer_mallet_lda <- function(slda_obj, word_probs, beta = 1, alpha = 100, niter = 10){
  # Initialize inputs
  ndocs <- slda_obj$dtm@Dim[[1]]
  nunique <- slda_obj$dtm@Dim[[2]]
  ntokens <- sum(slda_obj$dtm@x)
  nelem <- length(slda_obj$dtm@i)
  nlabs <- ncol(word_probs)
  return(infer_topics(slda_obj$dtm, ndocs, nunique, ntokens, nelem, nlabs, niter, beta, alpha, word_probs))
}



